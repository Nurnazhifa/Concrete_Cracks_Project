# -*- coding: utf-8 -*-
"""Assessment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SKdtvWmO-4Aqy7TN21Frxz5npql37IM2
"""

!pwd

#%%
#1. Import packages
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications
import numpy as np
import os
import matplotlib.pyplot as plt

#Step 1) To download the dataset:
!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip

#Step 2) Create a new folder call “dataset”:
#!makedir dataset

#Step 3) To unzip and extract into a new folder:
!unzip "5y9wdsg2zt-2.zip"
!unrar x "Concrete Crack Images for Classification.rar" "dataset"

#3. Data preparation
#(A) Define the path to the data folder 
PATH = os.path.join(os.getcwd(),'dataset')

#(B) Define the batch size and image size
BATCH_SIZE = 32
IMG_SIZE = (160,160)
SEED =42

#%%
#(C) Load the data into tensorflow dataset using the specific method

train_dataset = keras.utils.image_dataset_from_directory(PATH,validation_split=0.3,subset='training',seed = SEED,batch_size=BATCH_SIZE,image_size=IMG_SIZE,shuffle=True)
val_dataset = keras.utils.image_dataset_from_directory(PATH,validation_split=0.3,subset='validation',seed = SEED,batch_size=BATCH_SIZE,image_size=IMG_SIZE,shuffle=True)
class_names = train_dataset.class_names

# %%
#4. Display some images as example

plt.figure(figsize=(10,10))
for images,labels in train_dataset.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(images[i].numpy().astype('uint8'))
        plt.title(class_names[labels[i]])
        plt.axis('off')
# %%
#5. Further split the validation dataset into validation-test split
val_batches = tf.data.experimental.cardinality(val_dataset)
test_dataset = val_dataset.take(val_batches//5)
validation_dataset = val_dataset.skip(val_batches//5)
# %%
#6. Convert the BatchDataset into PrefetchDataset
AUTOTUNE = tf.data.AUTOTUNE

pf_train = train_dataset.prefetch(buffer_size=AUTOTUNE)
pf_val = validation_dataset.prefetch(buffer_size=AUTOTUNE)
pf_test = test_dataset.prefetch(buffer_size=AUTOTUNE)
# %%
#7. Create a small pipeline for data augmentation
data_augmentation = keras.Sequential()
data_augmentation.add(layers.RandomFlip('horizontal'))
data_augmentation.add(layers.RandomRotation(0.2))
# %%
#Apply the data augmentation to test it out
for images,labels in pf_train.take(1):
    first_image = images[0]
    plt.figure(figsize=(10,10))
    for i in range(9):
        plt.subplot(3,3,i+1)
        augmented_image = data_augmentation(tf.expand_dims(first_image,axis=0))
        plt.imshow(augmented_image[0]/255.0)
        plt.axis('off')
# %%
#8. Prepare the layer for data preprocessing
preprocess_input = applications.mobilenet_v2.preprocess_input

#9. Apply transfer learning
IMG_SHAPE = IMG_SIZE + (3,)
feature_extractor = applications.MobileNetV2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')

# Disable the training for the feature extractor (freeze the layers)
feature_extractor.trainable = False
feature_extractor.summary()
keras.utils.plot_model(feature_extractor,show_shapes=True)

# %%
#10. Create the classification layers
global_avg = layers.GlobalAveragePooling2D()
output_layer = layers.Dense(len(class_names),activation='softmax')
# %%
#11. Use functional API to link all of the modules together
inputs = keras.Input(shape=IMG_SHAPE)
x = data_augmentation(inputs)
x = preprocess_input(x)
x = feature_extractor(x)
x = global_avg(x)
x = layers.Dropout(0.3)(x)
outputs = output_layer(x)

model = keras.Model(inputs=inputs,outputs=outputs)
model.summary()
# %%
#12. Compile the model
optimizer = optimizers.Adam(learning_rate=0.0001)
loss = losses.SparseCategoricalCrossentropy()
model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])
# %%
#Evaluate the model before model training
loss0,accuracy0 = model.evaluate(pf_val)
print("Loss = ",loss0)
print("Accuracy = ",accuracy0)
# %%
import datetime
log_path = os.path.join('log_dir','folder',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
tb = callbacks.TensorBoard(log_dir=log_path)

# %%
#Train the model
EPOCHS = 10
history = model.fit(pf_train,validation_data=pf_val,epochs=EPOCHS,callbacks=[tb])

# %%
"""
Next, we are going to further fine tune the model by using a different transfer learning strategy --> fine tune pretrained model and frozen layers

What we are going to do is we are going to unfreeze some layers at the behind part of the feature extractor, so that those layers will be trained to extract the high features that we specifically want.
"""
#13. Apply the next transfer learning strategy
feature_extractor.trainable = True

# Freeze the earlier layers
for layer in feature_extractor.layers[:100]:
    layer.trainable = False

feature_extractor.summary()
# %%
#14. Compile the model
optimizer = optimizers.RMSprop(learning_rate=0.00001)
model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])
# %%
#15. Continue the model training with this new set of configuration
fine_tune_epoch = 10
total_epoch = EPOCHS + fine_tune_epoch

# Follow up from the previous model training
history_fine = model.fit(pf_train,validation_data=pf_val,epochs=total_epoch,initial_epoch=history.epoch[-1],callbacks=[tb])
# %%
#16. Evaluate the final model
test_loss,test_acc = model.evaluate(pf_test)

print("Loss = ",test_loss)
print("Accuracy = ",test_acc)
# %%
#Deploy the model using the test data
image_batch, label_batch = pf_test.as_numpy_iterator().next()
predictions = np.argmax(model.predict(image_batch),axis=1)

#Compare label and prediction
label_vs_prediction = np.transpose(np.vstack((label_batch,predictions)))
#

#%%
model.save('model.h5')